{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CM4603 â€“ Coursework 1 (Group)\n",
    "#### November 2023\n",
    "\n",
    "### Group: I\n",
    "\n",
    "Name 1: Kasun Abeyweera | \n",
    "IIT ID: 20211409 | \n",
    "RGU ID: 2121842\n",
    "\n",
    "Name 2: Sunera Udana | \n",
    "IIT ID:   | \n",
    "RGU ID: 2122099\n",
    "\n",
    "Name 3: Rikzy Mohamed | \n",
    "IIT ID:  | \n",
    "RGU ID: 2122107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## **Dataset Description**\n",
    "\n",
    "- **Size**: The dataset comprises **9,949 records**, each corresponding to a hotel review. Key features include:\n",
    "  - **`Customer`**: Unique identifier of the reviewer.\n",
    "  - **`text`**: Full text of the review.\n",
    "  - **`title`**: A short summary of the review.\n",
    "  - **`hotel_Name`**: Name of the hotel being reviewed.\n",
    "  - **`numberOfReviews`**: Total number of reviews provided by the customer.\n",
    "  - **`finalRating`**: Numerical rating on a scale from 1 to 5.\n",
    "\n",
    "---\n",
    "\n",
    "## **Initial Observations**\n",
    "\n",
    "- **Hotels Reviewed**: The dataset contains reviews for **140 unique hotels**.\n",
    "- **Review Distribution**:\n",
    "  - Mean number of reviews per hotel: **71**.\n",
    "  - The distribution of reviews per hotel shows a **skewed pattern**, ranging from **20 to 100 reviews per hotel**.\n",
    "\n",
    "- **Textual Characteristics**:\n",
    "  - **Total words**: **559,979**.\n",
    "  - **Unique words**: **42,501**.\n",
    "  - **Lexical Diversity**: **0.0759**.\n",
    "  - Average review length: **56.28 words**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Quality**\n",
    "\n",
    "- **Missing Values**:\n",
    "  - Only the `Customer` field has **48 missing entries**.\n",
    "  - The `title` column has **1 missing value**.\n",
    "  - All `text`, `hotel_Name`, and `finalRating` entries are complete.\n",
    "\n",
    "- Non-standard characters (e.g., emojis) and punctuation were observed in the text.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "\n",
    "### **1. Distribution of Review Lengths**\n",
    "\n",
    "- The distribution of review lengths, measured in the number of words, indicates that most reviews are short and concise, with a **peak around 56 words**.\n",
    "\n",
    "**Insights**:\n",
    "- The data has a small proportion of unusually long reviews.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Frequent Words in Reviews**\n",
    "\n",
    "\n",
    "\n",
    "- **Common Words**:\n",
    "  - Frequently used words include **\"hotel\" , \"staff\" , \"room\" , \"stay\" and \"food\"**.\n",
    "  - Frequently used Bi-grams include **\"sri lanka\" , \"higly recommennd\" , \"staff friendly\" , \"room clean\" and \"stayed night\"**.\n",
    "  - Frequently used Tri-grams include **\"staff friendly helpful\" , \"higly recommennd hotel\" , \"trip sri lanka\"**.\n",
    " \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Review rating distribution**\n",
    "\n",
    "- The distribution of rating, measured from 1 -5, indicates that most reviews are in rate 5.\n",
    "\n",
    "**Insights**:\n",
    "- The data can be more lean towards the postive sentiments.\n",
    " \n",
    "---\n",
    "\n",
    "## **Tokenization Analysis**\n",
    "\n",
    "To analyze and process text effectively, we compared five popular tokenizers based on speed, handling of emojis, and overall accuracy. The tokenizers were tested on the dataset for their token count, unique tokens, average tokens per review, and runtime.\n",
    "\n",
    "| **Tokenizer**         | **Time (s)** | **Tokens** | **Unique Tokens** | **Tokens/Review** |\n",
    "|------------------------|---------------|------------|--------------------|-------------------|\n",
    "| `nltk_word_tokenize`   | **3.1539 s** | **890,557**| **45,807**         | **98.95**         |\n",
    "| `treebank_tokenizer`   | **1.2277 s** | **841,292**| **53,487**         | **93.48**         |\n",
    "| `whitespace_tokenizer` | **0.2121 s** | **788,738**| **66,384**         | **87.64**         |\n",
    "| **`tweet_tokenizer`**  | **2.9873 s** | **895,349**| **44,976**         | **99.48**         |\n",
    "| `spacy_tokenizer`      | **4660.25 s**| **916,500**| **43,709**         | **101.83**        |\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison and Selection**\n",
    "\n",
    "#### **1. `nltk_word_tokenize`**\n",
    "- Generated **45,807 unique tokens**.\n",
    "- Runtime: **3.15 s**.\n",
    "- Average tokens per review: **98.95**.\n",
    "- Observations:\n",
    "  - Balanced speed and token count.\n",
    "  - Struggled with handling emojis and special characters.\n",
    "\n",
    "#### **2. `treebank_tokenizer`**\n",
    "- Generated **53,487 unique tokens**, slightly more than `nltk_word_tokenize`.\n",
    "- Runtime: **1.22 s** (fastest of the advanced methods).\n",
    "- Average tokens per review: **93.48**.\n",
    "- Observations:\n",
    "  - Handled punctuation better but still struggled with emojis and informal text.\n",
    "\n",
    "#### **3. `whitespace_tokenizer`**\n",
    "- Generated the **most unique tokens**: **66,384**.\n",
    "- Runtime: **0.21 s** (fastest overall).\n",
    "- Average tokens per review: **87.64**.\n",
    "- Observations:\n",
    "  - Over-split text, producing an unusually high unique token count.\n",
    "  - Insufficient accuracy for rich, sentiment-heavy text.\n",
    "\n",
    "#### **4. `TweetTokenizer`**\n",
    "- Generated **44,976 unique tokens**, close to `nltk_word_tokenize`.\n",
    "- Runtime: **2.98 s**.\n",
    "- Average tokens per review: **99.48**.\n",
    "- Observations:\n",
    "  - Handled emojis, hashtags, and abbreviations effectively.\n",
    "  - Balanced speed and tokenization accuracy.\n",
    "\n",
    "#### **5. `SpaCy Tokenizer`**\n",
    "- Generated **43,709 unique tokens** (least among methods).\n",
    "- Runtime: **4,660 s** (significantly slower than others).\n",
    "- Average tokens per review: **101.83**.\n",
    "- Observations:\n",
    "  - Most robust tokenization.\n",
    "  - Computationally expensive, unsuitable for this dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Chosen Tokenizer: `TweetTokenizer`**\n",
    "\n",
    "- **Reason for Selection**:\n",
    "  - Balances speed and accuracy.\n",
    "  - Effectively handles **emojis** and **special characters**, which are prevalent in hotel reviews.\n",
    "  - Better for informal text handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, TreebankWordTokenizer, WhitespaceTokenizer, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('master-dataset.csv')\n",
    "\n",
    "print(\"Initial Data Shape:\", data.shape)\n",
    "print(\"Missing Values per Column:\\n\", data.isnull().sum())\n",
    "data.dropna(subset=['text'], inplace=True)  \n",
    "print(\"Data Shape after Dropping Missing Text:\", data.shape)\n",
    "\n",
    "# Define tokenizer evaluation function\n",
    "def evaluate_tokenizer_performance(tokenizers, text_samples):\n",
    "    results = []\n",
    "    for name, tokenizer in tokenizers.items():\n",
    "        start_time = time.time()\n",
    "        tokens = [tokenizer(text) for text in text_samples]\n",
    "        duration = time.time() - start_time\n",
    "        total_tokens = sum(len(tok) for tok in tokens)\n",
    "        unique_tokens = len(set(word for tok in tokens for word in tok))\n",
    "        results.append({\n",
    "            'Tokenizer': name,\n",
    "            'Time (s)': round(duration, 4),\n",
    "            'Total Tokens': total_tokens,\n",
    "            'Unique Tokens': unique_tokens,\n",
    "            'Tokens per Review': round(total_tokens / len(text_samples), 2)\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "text_samples = data['text'].sample(10, random_state=42).tolist()\n",
    "tokenizers = {\n",
    "    'nltk_word_tokenize': lambda x: word_tokenize(x.lower()),\n",
    "    'treebank_tokenizer': lambda x: TreebankWordTokenizer().tokenize(x.lower()),\n",
    "    'whitespace_tokenizer': lambda x: WhitespaceTokenizer().tokenize(x.lower()),\n",
    "    'tweet_tokenizer': lambda x: TweetTokenizer().tokenize(x.lower()),\n",
    "    'spacy_tokenizer': lambda x: [token.text.lower() for token in spacy.load(\"en_core_web_sm\")(x)]\n",
    "}\n",
    "\n",
    "results_df = evaluate_tokenizer_performance(tokenizers, text_samples)\n",
    "\n",
    "print(\"\\nTokenizer Performance:\")\n",
    "display(results_df)\n",
    "\n",
    "sample_text = '''Wonderful experience ðŸ˜ friendly staff, delicious food, beautiful view. And their have arranged our 3 meals in 3 beautiful location with breathtaking view, and we loved this experience , and should mention the trekking experience we had and we went to a beautiful waterfall and that was surprise to us and this amazing place is highly recommended.'''\n",
    "def tokenize_methods(text):\n",
    "    tokenizations = {}\n",
    "    tokenizations['word_tokenize'] = word_tokenize(text.lower())\n",
    "    tokenizations['treebank'] = TreebankWordTokenizer().tokenize(text.lower())\n",
    "    tokenizations['whitespace'] = WhitespaceTokenizer().tokenize(text.lower())\n",
    "    tokenizations['tweet_tokenizer'] = TweetTokenizer().tokenize(text.lower())\n",
    "    tokenizations['spacy'] = [token.text.lower() for token in spacy.load(\"en_core_web_sm\")(text)]\n",
    "    return tokenizations\n",
    "\n",
    "print(\"\\nTokenization Examples:\")\n",
    "for method, tokens in tokenize_methods(sample_text).items():\n",
    "    print(f\"{method}: {tokens[:20]}...\")\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = [tweet_tokenizer.tokenize(sentence.lower()) for sentence in sentences]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_words = [\n",
    "        [lemmatizer.lemmatize(word) for word in sentence if word not in stop_words and word.isalpha()]\n",
    "        for sentence in words\n",
    "    ]\n",
    "    return ' '.join(word for sentence in processed_words for word in sentence)\n",
    "\n",
    "data['cleaned_text'] = data[['title', 'text']].apply(lambda row: preprocess_text(f\"{row['title']} {row['text']}\"), axis=1)\n",
    "print(\"\\nCleaned Data Sample:\")\n",
    "display(data[['title', 'text', 'cleaned_text']].head())\n",
    "\n",
    "# Basic dataset statistics\n",
    "print(f\"\\nTotal hotels: {data['hotel_Name'].nunique()}\")\n",
    "print(f\"Total words: {sum(len(x.split()) for x in data['cleaned_text'])}\")\n",
    "print(f\"Unique words: {len(set(' '.join(data['cleaned_text']).split()))}\")\n",
    "\n",
    "# Number of reviews per hotel\n",
    "reviews_per_hotel = data.groupby('hotel_Name').size()\n",
    "print(\"\\nNumber of reviews per hotel (Summary):\")\n",
    "print(reviews_per_hotel.describe())\n",
    "\n",
    "all_words = [word for review in data['cleaned_text'] for word in review.split()]\n",
    "unique_words = set(all_words)\n",
    "\n",
    "# Compute lexical diversity\n",
    "lexical_diversity = len(unique_words) / len(all_words)\n",
    "print(f\"Lexical Diversity: {lexical_diversity:.4f}\")\n",
    "\n",
    "# EDA\n",
    "\n",
    "# Histogram of review lengths\n",
    "data['review_length'] = data['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate average review length\n",
    "average_review_length = data['review_length'].mean()\n",
    "\n",
    "# Plot histogram of review lengths\n",
    "sns.histplot(data['review_length'], bins=20, kde=True, color='blue')\n",
    "plt.title(f'Distribution of Review Lengths (Avg: {average_review_length:.2f} words)')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(average_review_length, color='red', linestyle='--', label=f'Avg: {average_review_length:.2f}')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Word Cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data['cleaned_text']))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Bi-Gram Analysis\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english')\n",
    "bi_grams = vectorizer.fit_transform(data['cleaned_text'])\n",
    "bi_gram_counts = pd.DataFrame(bi_grams.sum(axis=0), columns=vectorizer.get_feature_names_out(), index=['count']).T\n",
    "bi_gram_counts = bi_gram_counts.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# Visualize Bi-Grams\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=bi_gram_counts.index, y=bi_gram_counts['count'], color='blue')\n",
    "plt.title(\"Top 10 Bi-Grams in Reviews\")\n",
    "plt.xlabel(\"Bi-Grams\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Tri-Gram Analysis\n",
    "vectorizer_trigram = CountVectorizer(ngram_range=(3, 3), stop_words='english')\n",
    "tri_grams = vectorizer_trigram.fit_transform(data['cleaned_text'])\n",
    "tri_gram_counts = pd.DataFrame(tri_grams.sum(axis=0), columns=vectorizer_trigram.get_feature_names_out(), index=['count']).T\n",
    "tri_gram_counts = tri_gram_counts.sort_values(by='count', ascending=False).head(10)\n",
    "\n",
    "# Visualize Top Tri-Grams\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=tri_gram_counts.index, y=tri_gram_counts['count'], color='green')\n",
    "plt.title(\"Top 10 Tri-Grams in Reviews\")\n",
    "plt.xlabel(\"Tri-Grams\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "if 'finalRating' in data.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for rating in data['finalRating'].unique():\n",
    "        words_in_rating = ' '.join(data[data['finalRating'] == rating]['cleaned_text']).split()\n",
    "        word_counts = Counter(words_in_rating)\n",
    "        most_common = word_counts.most_common(10)\n",
    "        words, counts = zip(*most_common)\n",
    "        plt.bar(words, counts, label=f'Rating {rating}')\n",
    "    plt.title('Most Frequent Words by Rating')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Distribution of Ratings per Hotel\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='finalRating', data=data, palette='viridis')\n",
    "plt.title('Ratings Distribution')\n",
    "plt.xlabel('Final Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Reviews Per Hotel\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(reviews_per_hotel, bins=20, kde=False, color='purple')\n",
    "plt.title('Number of Reviews Per Hotel')\n",
    "plt.xlabel('Reviews Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Save preprocessed dataset\n",
    "data.to_csv('preprocessed_dataset.csv', index=False)\n",
    "print(\"\\nPreprocessed dataset saved as 'preprocessed_dataset.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Task 2\n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "#### Raw Classification Dataset\n",
    "- **Dimensions**:  \n",
    "  The raw dataset contains the following columns with 9949 rows:  \n",
    "  - `Customer`: Identifier for the customer who provided the review.  \n",
    "  - `text`: The full review text written by the customer.  \n",
    "  - `title`: Title of the review.  \n",
    "  - `hotel_Name`: Name of the hotel being reviewed.  \n",
    "  - `numberOfReviews`: Number of reviews for the hotel (aggregated from all users).  \n",
    "  - `finalRating`: The rating given by the user (e.g., 1 to 5).  \n",
    "  - `cleaned_text`: Processed version of the `text` field after cleaning.  \n",
    "  - `review_length`: The number of words in the cleaned review text.  \n",
    "\n",
    "- **Dataset Preprocessing steps included**\n",
    "- The dataset was preprocessed to clean and normalize the text for analysis:\n",
    "  - Combined the `title` and `text` fields into a unified `cleaned_text` column.\n",
    "  - Removed stopwords and applied lemmatization to ensure uniformity.\n",
    "  - Tokenized the text using the **TweetTokenizer** to handle emojis, punctuation, and informal text effectively.\n",
    "\n",
    "- **Classes for Classification**:  \n",
    "  We performed **ternary classification** into three sentiment categories:  \n",
    "  - `positive`  \n",
    "  - `negative`  \n",
    "  - `neutral`  \n",
    "\n",
    "- **Feature Extraction Techniques**:  \n",
    "  - **Sentiment Classification**: Applied three different sentiment classifiers:\n",
    "    - VADER Sentiment Analysis.\n",
    "    - Bing Liu's Lexicon-based method.\n",
    "    - TextBlob Sentiment Analysis.\n",
    "  - **Majority Voting**: Combined the outputs from the classifiers to establish the `ground_truth_sentiment` for each review.\n",
    "  - **Agreement Check**: Added an `agreement` column to measure the consistency between classifier outputs.\n",
    "\n",
    "### Ground Truth Generation: Documentation and Validation\n",
    "\n",
    "#### Process for Determining Ground Truth\n",
    "1. **Classifier Selection**:  \n",
    "   To establish the sentiment ground truth, three sentiment classifiers were chosen:\n",
    "   - **VADER Sentiment Analysis**: A lexicon and rule-based sentiment analysis tool tailored for social media and review text.\n",
    "   - **Bing Liu's Lexicon**: A sentiment lexicon with lists of positive and negative words for domain-independent sentiment analysis.\n",
    "   - **TextBlob Sentiment Analysis**: A polarity-based sentiment analysis tool that uses text processing techniques to determine sentiment.\n",
    "\n",
    "2. **Majority Voting Mechanism**:  \n",
    "   - Each review's sentiment was classified as `positive`, `negative`, or `neutral` by all three classifiers.  \n",
    "   - The **final sentiment label (`ground_truth_sentiment`)** was assigned based on **majority voting**:\n",
    "     - If two or more classifiers agreed on a sentiment, that sentiment was chosen.\n",
    "\n",
    "3. **Agreement Levels**:  \n",
    "   - A column named `agreement` was added to measure consistency among classifiers.\n",
    "   - High agreement levels indicate stronger confidence in the ground truth sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "#### Justification of the Ground Truth Methodology\n",
    "1. **Combining Multiple Classifiers**:  \n",
    "   - Using multiple classifiers ensures robustness in sentiment determination by leveraging different methods:\n",
    "     - VADER: Excels in handling polarity and negation in texts.\n",
    "     - Bing Liu Lexicon: Provides a domain-independent approach with established word lists.\n",
    "     - TextBlob: Adds polarity-based precision for nuanced reviews.\n",
    "   - This mitigates the limitations of any single classifier.\n",
    "\n",
    "2. **Majority Voting**:  \n",
    "   - Provides a simple yet effective way to integrate multiple perspectives into a single sentiment label.\n",
    "   - By relying on consensus, it minimizes the impact of potential biases or errors from individual classifiers.\n",
    "\n",
    "3. **Granularity of Sentiment Classes**:  \n",
    "   - Ternary classification (`positive`, `negative`, `neutral`) captures neutral opinions, which are significant in hotel reviews.\n",
    "   - This is justified over binary classification (`positive` and `negative`) to retain nuanced feedback, important for understanding customer experiences.\n",
    "\n",
    "---\n",
    "\n",
    "#### Validation of Ground Truth\n",
    "1. **Agreement Analysis**:  \n",
    "   - Analyzed the `agreement` column to ensure high consistency across classifiers.\n",
    "   - Reviews with low agreement were inspected to assess potential classification ambiguities.\n",
    "\n",
    "2. **Distribution Analysis**:  \n",
    "   - The distribution of `ground_truth_sentiment` labels was plotted to ensure balanced sentiment categories.\n",
    "   - More positive reviews were observed . (8892)\n",
    "\n",
    "3. **Comparison with Ratings**:  \n",
    "   - The `ground_truth_sentiment` was cross-checked with numerical ratings (`finalRating`) to validate consistency:\n",
    "     - High ratings (e.g., 4â€“5 stars) generally aligned with `positive` sentiment.\n",
    "     - Low ratings (e.g., 1â€“2 stars) aligned with `negative` sentiment.\n",
    "     - Mid-range ratings (e.g., 3 stars) often corresponded to `neutral` sentiment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    data = pd.read_csv('preprocessed_dataset.csv')\n",
    "    print(\"Dataset Loaded!\")\n",
    "    print(data.head())\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(\"Could not load the dataset. Please ensure 'preprocessed_dataset.csv' exists.\") from e\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load Bing Liu Lexicon\n",
    "try:\n",
    "    positive_words = set(open('positive-words.txt').read().splitlines())\n",
    "    negative_words = set(open('negative-words.txt').read().splitlines())\n",
    "except Exception as e:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not load Bing Liu Lexicon files. Ensure 'positive-words.txt' and 'negative-words.txt' exist.\") from e\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing stopwords, punctuation, and converting to lowercase.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \" \".join(word for word in tokens if word not in stop_words)\n",
    "\n",
    "\n",
    "def classify_vader(text):\n",
    "    \"\"\"Classify sentiment using VADER.\"\"\"\n",
    "    score = sia.polarity_scores(text)\n",
    "    if score['compound'] > 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "def classify_bing_liu(text):\n",
    "    \"\"\"Classify sentiment using Bing Liu's Lexicon.\"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_count = sum(1 for word in tokens if word in positive_words)\n",
    "    neg_count = sum(1 for word in tokens if word in negative_words)\n",
    "\n",
    "    if pos_count > neg_count:\n",
    "        return 'positive'\n",
    "    elif neg_count > pos_count:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "def classify_textblob(text):\n",
    "    \"\"\"Classify sentiment using TextBlob.\"\"\"\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return 'positive'\n",
    "    elif polarity < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "\n",
    "def majority_vote(votes):\n",
    "    \"\"\"Return the sentiment with the most votes.\"\"\"\n",
    "    return Counter(votes).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "data['cleaned_text'] = data['cleaned_text'].apply(clean_text)\n",
    "\n",
    "print(\"Applying sentiment classifiers...\")\n",
    "data['vader_sentiment'] = data['cleaned_text'].apply(classify_vader)\n",
    "data['bing_liu_sentiment'] = data['cleaned_text'].apply(classify_bing_liu)\n",
    "data['textblob_sentiment'] = data['cleaned_text'].apply(classify_textblob)\n",
    "\n",
    "# Determine ground truth sentiment via majority voting\n",
    "data['ground_truth_sentiment'] = data.apply(\n",
    "    lambda row: majority_vote([row['vader_sentiment'], row['bing_liu_sentiment'], row['textblob_sentiment']]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Ground Truth Sentiment Distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.countplot(x='ground_truth_sentiment', data=data, palette='viridis')\n",
    "plt.title('Ground Truth Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Agreement Levels Between Classifiers\n",
    "data['agreement'] = data.apply(\n",
    "    lambda row: len(set([row['vader_sentiment'], row['bing_liu_sentiment'], row['textblob_sentiment']])),\n",
    "    axis=1\n",
    ")\n",
    "print(\"Agreement Levels:\\n\", data['agreement'].value_counts())\n",
    "\n",
    "# Article Length Histogram\n",
    "data['text_length'] = data['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(data['text_length'], bins=50, kde=True, color='blue')\n",
    "plt.title('Article Length Distribution')\n",
    "plt.xlabel('Text Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "data.to_csv('ground_truth_sentiment_dataset.csv', index=False)\n",
    "print(\"Ground truth sentiment dataset saved as 'ground_truth_sentiment_dataset.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Feature Set Description\n",
    "\n",
    "After classification and feature extraction, the dataset includes the following additional columns:\n",
    "- **`vader_sentiment`**: Sentiment output from the VADER classifier.  \n",
    "- **`bing_liu_sentiment`**: Sentiment output from Bing Liu's Lexicon.  \n",
    "- **`textblob_sentiment`**: Sentiment output from TextBlob.  \n",
    "- **`ground_truth_sentiment`**: Final sentiment label determined by majority voting.  \n",
    "- **`agreement`**: Count of unique sentiments across the three classifiers for each review.  \n",
    "- **`text_length`**: Length of the review text (in words) after cleaning.\n",
    "\n",
    "#### Final Feature Set Shapes\n",
    "- **Number of Rows**: Matches the original dataset size (one row per review).  \n",
    "- **Number of Features**: Increased to include the sentiment analysis outputs and meta-information:\n",
    "  - Original columns: `Customer`, `text`, `title`, `hotel_Name`, `numberOfReviews`, `finalRating`, `cleaned_text`, `review_length`.\n",
    "  - Added columns: `vader_sentiment`, `bing_liu_sentiment`, `textblob_sentiment`, `ground_truth_sentiment`, `agreement`, `text_length`.  \n",
    "\n",
    "#### Final Dataset example \n",
    "| Customer      | Text                         | Hotel Name                | Final Rating | Cleaned Text       | Review Length | VADER Sentiment | Bing Liu Sentiment | TextBlob Sentiment | Ground Truth Sentiment | Agreement | Text Length |\n",
    "|---------------|------------------------------|---------------------------|--------------|--------------------|---------------|-----------------|--------------------|--------------------|------------------------|-----------|-------------|\n",
    "| lauradog95    | I had a wonderful stay...    | Colombo Court Hotel & Spa | 5            | fantastic...        | 39            | Positive        | Positive           | Positive           | Positive               | 1         | 39          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer to Task 3\n",
    "\n",
    "### Feature Extraction Summary\n",
    "\n",
    "In this project, we explored four distinct methods to extract meaningful features from the text data for sentiment analysis. These methods provide a balance between sparse and dense representations:\n",
    "\n",
    "1. **Bag of Words (BoW):**  \n",
    "   - **Key Hyperparameters:**  \n",
    "     - `max_features=40000` to focus on the most frequent words and reduce computational cost.\n",
    "\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency):**  \n",
    "   - **Key Hyperparameters:**  \n",
    "     - `max_features=40000` to capture essential vocabulary without overloading the model.\n",
    "\n",
    "3. **Word2Vec:**   \n",
    "   - **Key Hyperparameters:**  \n",
    "     - `vector_size=300` to define the dimensionality of embeddings.  \n",
    "     - `window=8` to consider a broader context for each word.  \n",
    "     - `min_count=1` to include rare words in the model.  \n",
    "\n",
    "4. **Doc2Vec:**  \n",
    "   - **Key Hyperparameters:**  \n",
    "     - `vector_size=300` for the embedding size.  \n",
    "     - `window=8` for context size.  \n",
    "     - `min_count=1` to include all words.         \n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('ground_truth_sentiment_dataset.csv')\n",
    "\n",
    "# Bag of Words (BoW)\n",
    "print(\"Extracting Bag of Words (BoW)...\")\n",
    "bow_vectorizer = CountVectorizer(max_features=40000) \n",
    "bow_features = bow_vectorizer.fit_transform(data['cleaned_text'])\n",
    "print(f\"BoW Shape: {bow_features.shape}\")\n",
    "np.save('bow_features.npy', bow_features.toarray())\n",
    "\n",
    "# TF-IDF\n",
    "print(\"Extracting TF-IDF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=40000)  \n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n",
    "print(f\"TF-IDF Shape: {tfidf_features.shape}\")\n",
    "np.save('tfidf_features.npy', tfidf_features.toarray())\n",
    "\n",
    "# Word2Vec\n",
    "print(\"Training Word2Vec...\")\n",
    "word2vec_model = Word2Vec(\n",
    "    sentences=data['cleaned_text'].str.split(),\n",
    "    vector_size=300,  # Size of word embeddings\n",
    "    window=8,         # Context window size\n",
    "    min_count=1,      # Minimum word frequency\n",
    "    workers=4         # Number of threads\n",
    ")\n",
    "print(\"Generating Word2Vec document embeddings...\")\n",
    "word2vec_features = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in text.split() if word in word2vec_model.wv] or\n",
    "            [np.zeros(word2vec_model.vector_size)], axis=0)\n",
    "    for text in data['cleaned_text']\n",
    "])\n",
    "print(f\"Word2Vec Shape: {word2vec_features.shape}\")\n",
    "np.save('word2vec_features.npy', word2vec_features)\n",
    "\n",
    "# Doc2Vec\n",
    "print(\"Training Doc2Vec...\")\n",
    "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(data['cleaned_text'])]\n",
    "doc2vec_model = Doc2Vec(\n",
    "    tagged_data,\n",
    "    vector_size=300,  # Size of document embeddings\n",
    "    window=8,         # Context window size\n",
    "    min_count=1,      # Minimum word frequency\n",
    "    workers=4,        # Number of threads\n",
    "    epochs=50         # Increased epochs for better convergence\n",
    ")\n",
    "print(\"Generating Doc2Vec document embeddings...\")\n",
    "doc2vec_features = np.array([doc2vec_model.infer_vector(text.split()) for text in data['cleaned_text']])\n",
    "print(f\"Doc2Vec Shape: {doc2vec_features.shape}\")\n",
    "np.save('doc2vec_features.npy', doc2vec_features)\n",
    "\n",
    "# Save all features into a single file for convenience\n",
    "data.to_csv('feature_extracted_dataset.csv', index=False)\n",
    "print(\"Features saved!\")\n",
    "\n",
    "# Visualize Word2Vec Features with t-SNE\n",
    "print(\"Visualizing Word2Vec features...\")\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(word2vec_features)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], alpha=0.5)\n",
    "plt.title('Word2Vec Feature Visualization')\n",
    "plt.xlabel('TSNE Component 1')\n",
    "plt.ylabel('TSNE Component 2')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Answer to Task 4\n",
    "\n",
    "## Justification for Using the Selected Non-Deep Learning Algorithms\n",
    "\n",
    "1. **Logistic Regression (LR):**\n",
    "   - Logistic Regression is a simple yet effective linear classifier, particularly well-suited for high-dimensional, sparse data like Bag of Words (BoW) and TF-IDF. \n",
    "   - Its interpretability and ability to model probabilities make it a widely-used choice for text classification tasks.\n",
    "   - LR is most effective when the decision boundary is linear and the data is reasonably separable, as seen in the results.\n",
    "\n",
    "2. **Support Vector Machine (SVM):**\n",
    "   - SVM excels in cases where the data is not perfectly separable, leveraging the concept of margin maximization.\n",
    "   - By using a linear kernel, SVM efficiently handles high-dimensional data such as TF-IDF features, capturing nuanced boundaries between classes.\n",
    "   - SVM's robustness against overfitting makes it a strong candidate for scenarios involving complex feature transformations.\n",
    "\n",
    "3. **Random Forest (RF):**\n",
    "   - Random Forest is an ensemble method that combines multiple decision trees to improve generalization and reduce variance.\n",
    "   - RF works particularly well with dense vector representations like Word2Vec and Doc2Vec, as it can effectively capture the semantic relationships embedded in these features.\n",
    "   - Its ability to avoid overfitting while delivering consistent performance on dense embeddings makes it a reliable choice.\n",
    "\n",
    "4. **Naive Bayes (NB):**\n",
    "   - Although NB was not applied to dense features like Word2Vec and Doc2Vec due to mathematical limitations, it performs efficiently on sparse features like BoW and TF-IDF.\n",
    "   - NB's independence assumption aligns well with the nature of these feature extraction methods, delivering competitive results with minimal computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load features\n",
    "bow_features = np.load('bow_features.npy')\n",
    "tfidf_features = np.load('tfidf_features.npy')\n",
    "word2vec_features = np.load('word2vec_features.npy')\n",
    "doc2vec_features = np.load('doc2vec_features.npy')\n",
    "\n",
    "# Load labels\n",
    "data = pd.read_csv('feature_extracted_dataset.csv')\n",
    "y = data['ground_truth_sentiment']\n",
    "\n",
    "# Split for each feature set\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(bow_features, y, test_size=0.2, random_state=42)\n",
    "X_train_tfidf, X_test_tfidf, _, _ = train_test_split(tfidf_features, y, test_size=0.2, random_state=42)\n",
    "X_train_word2vec, X_test_word2vec, _, _ = train_test_split(word2vec_features, y, test_size=0.2, random_state=42)\n",
    "X_train_doc2vec, X_test_doc2vec, _, _ = train_test_split(doc2vec_features, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to train and evaluate a model\n",
    "def train_and_evaluate(model, X_train, X_test, y_train, y_test, feature_name, model_name):\n",
    "    \"\"\"Train and evaluate a model.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Accuracy and classification report\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{model_name} with {feature_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "    plt.title(f\"Confusion Matrix: {model_name} with {feature_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Logistic Regression with default hyperparameters\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "print(\"Logistic Regression with BoW\")\n",
    "acc_lr_bow = train_and_evaluate(lr, X_train_bow, X_test_bow, y_train, y_test, \"BoW\", \"Logistic Regression\")\n",
    "\n",
    "print(\"Logistic Regression with Word2Vec\")\n",
    "acc_lr_word2vec = train_and_evaluate(lr, X_train_word2vec, X_test_word2vec, y_train, y_test, \"Word2Vec\", \"Logistic Regression\")\n",
    "\n",
    "print(\"Logistic Regression with Doc2Vec\")\n",
    "acc_lr_doc2vec = train_and_evaluate(lr, X_train_doc2vec, X_test_doc2vec, y_train, y_test, \"Doc2Vec\", \"Logistic Regression\")\n",
    "\n",
    "# SVM with default hyperparameters\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "\n",
    "print(\"SVM with TF-IDF\")\n",
    "acc_svm_tfidf = train_and_evaluate(svm, X_train_tfidf, X_test_tfidf, y_train, y_test, \"TF-IDF\", \"SVM\")\n",
    "\n",
    "print(\"SVM with Word2Vec\")\n",
    "acc_svm_word2vec = train_and_evaluate(svm, X_train_word2vec, X_test_word2vec, y_train, y_test, \"Word2Vec\", \"SVM\")\n",
    "\n",
    "print(\"SVM with Doc2Vec\")\n",
    "acc_svm_doc2vec = train_and_evaluate(svm, X_train_doc2vec, X_test_doc2vec, y_train, y_test, \"Doc2Vec\", \"SVM\")\n",
    "\n",
    "# Random Forest with default hyperparameters\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Random Forest with Word2Vec\")\n",
    "acc_rf_word2vec = train_and_evaluate(rf, X_train_word2vec, X_test_word2vec, y_train, y_test, \"Word2Vec\", \"Random Forest\")\n",
    "\n",
    "print(\"Random Forest with Doc2Vec\")\n",
    "acc_rf_doc2vec = train_and_evaluate(rf, X_train_doc2vec, X_test_doc2vec, y_train, y_test, \"Doc2Vec\", \"Random Forest\")\n",
    "\n",
    "# Naive Bayes with BoW and TF-IDF\n",
    "nb = MultinomialNB()\n",
    "\n",
    "print(\"Naive Bayes with BoW\")\n",
    "acc_nb_bow = train_and_evaluate(nb, X_train_bow, X_test_bow, y_train, y_test, \"BoW\", \"Naive Bayes\")\n",
    "\n",
    "print(\"Naive Bayes with TF-IDF\")\n",
    "acc_nb_tfidf = train_and_evaluate(nb, X_train_tfidf, X_test_tfidf, y_train, y_test, \"TF-IDF\", \"Naive Bayes\")\n",
    "\n",
    "# Summarize Results\n",
    "results = {\n",
    "    'Feature': ['BoW', 'TF-IDF', 'Word2Vec', 'Doc2Vec'],\n",
    "    'Logistic Regression': [acc_lr_bow, None, acc_lr_word2vec, acc_lr_doc2vec],\n",
    "    'SVM': [None, acc_svm_tfidf, acc_svm_word2vec, acc_svm_doc2vec],\n",
    "    'Random Forest': [None, None, acc_rf_word2vec, acc_rf_doc2vec],\n",
    "    'Naive Bayes': [acc_nb_bow, acc_nb_tfidf, None, None]  # Not compatible with dense features\n",
    "}\n",
    "\n",
    "# Print and plot the updated results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "# Plot accuracy comparison\n",
    "results_df.set_index('Feature').plot(kind='bar', figsize=(12, 6))\n",
    "plt.title(\"Model Performance by Feature Extraction Method\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(title=\"Models\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison and Interpretation of Results\n",
    "\n",
    "| Feature   | Logistic Regression | SVM        | Random Forest | Naive Bayes |\n",
    "|-----------|----------------------|------------|---------------|-------------|\n",
    "| **BoW**   | 0.95                | N/A        | N/A           | 0.90        |\n",
    "| **TF-IDF**| N/A                 | 0.93       | N/A           | 0.90        |\n",
    "| **Word2Vec** | 0.92             | 0.92       | 0.92          | N/A         |\n",
    "| **Doc2Vec** | 0.89             | 0.89       | 0.89          | N/A         |\n",
    "\n",
    "### Observations:\n",
    "1. **BoW:**\n",
    "   - Logistic Regression achieves the highest accuracy of 0.95, demonstrating its ability to exploit the sparse, linear structure of the BoW feature set.\n",
    "   - Naive Bayes performs adequately with an accuracy of 0.90, consistent with its assumptions about feature independence.\n",
    "\n",
    "2. **TF-IDF:**\n",
    "   - SVM outperforms Naive Bayes with an accuracy of 0.93 compared to 0.90. This suggests SVM's capability to better utilize the weighted feature importance provided by TF-IDF.\n",
    "\n",
    "3. **Word2Vec:**\n",
    "   - Logistic Regression, SVM, and Random Forest achieve comparable accuracies (~0.92). This indicates that the semantic richness of Word2Vec embeddings is well-utilized by these models.\n",
    "   - Random Forestâ€™s ensemble strategy proves effective in making robust decisions on dense embeddings.\n",
    "\n",
    "4. **Doc2Vec:**\n",
    "   - All three algorithms (Logistic Regression, SVM, and Random Forest) achieve an accuracy of 0.89, which is slightly lower than Word2Vec.\n",
    "   - This reflects the differences in semantic representation: Doc2Vec embeddings may not capture class-specific nuances as effectively as Word2Vec.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Answer to Task 5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
